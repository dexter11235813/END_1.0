{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "END_assignment13.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyONwxnY4JqQqak21Ul4nFYL",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dexter11235813/END_1.0/blob/main/assignment_13/END_assignment13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpUl6NBam0AO"
      },
      "source": [
        "# **Downloading requisite files / packages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4Sewm1lmn-w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79241d5b-9f89-4b4e-b201-2e868e0a21b6"
      },
      "source": [
        "!wget \"http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\"\r\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-25 18:55:45--  http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\n",
            "Resolving www.cs.cornell.edu (www.cs.cornell.edu)... 132.236.207.36\n",
            "Connecting to www.cs.cornell.edu (www.cs.cornell.edu)|132.236.207.36|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9916637 (9.5M) [application/zip]\n",
            "Saving to: ‘cornell_movie_dialogs_corpus.zip.1’\n",
            "\n",
            "cornell_movie_dialo 100%[===================>]   9.46M  15.2MB/s    in 0.6s    \n",
            "\n",
            "2021-02-25 18:55:46 (15.2 MB/s) - ‘cornell_movie_dialogs_corpus.zip.1’ saved [9916637/9916637]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6vAL8Kmmvv1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64285e0b-04de-4158-b8b3-b70e61407424"
      },
      "source": [
        "!unzip \"cornell_movie_dialogs_corpus.zip\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  cornell_movie_dialogs_corpus.zip\n",
            "replace cornell movie-dialogs corpus/.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sezZIxxmwoV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5eb52f37-df0a-41f2-a227-28ca65ed39cf"
      },
      "source": [
        "!python -m spacy download en\r\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (53.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLhW6BncnBLN"
      },
      "source": [
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "import torch.nn.functional as F\r\n",
        "import torchtext\r\n",
        "from torchtext.datasets import Multi30k\r\n",
        "from torchtext.data import Field, BucketIterator, Iterator\r\n",
        "from torchtext import data\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import matplotlib.ticker as ticker\r\n",
        "\r\n",
        "import spacy\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "import random\r\n",
        "import math\r\n",
        "import time"
      ],
      "execution_count": 335,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bia41QfKnBJE"
      },
      "source": [
        "\r\n",
        "SEED = 42\r\n",
        "device = 'cuda'\r\n",
        "random.seed(SEED)\r\n",
        "np.random.seed(SEED)\r\n",
        "torch.manual_seed(SEED)\r\n",
        "torch.cuda.manual_seed(SEED)\r\n",
        "torch.backends.cudnn.deterministic = True\r\n",
        "\r\n"
      ],
      "execution_count": 639,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqJjdgyWnBGt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "ac0d4963-18bd-47d6-8bfc-a6dbe27a71da"
      },
      "source": [
        "import pandas as pd\r\n",
        "df=pd.read_csv(\"/content/cornell movie-dialogs corpus/movie_lines.txt\", sep='\\+\\+\\+\\$\\+\\+\\+', header=None)\r\n",
        "df.columns = ['line_id','char_id', 'movie_id', 'char_name', 'dialog' ]\r\n",
        "df.head()"
      ],
      "execution_count": 640,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>line_id</th>\n",
              "      <th>char_id</th>\n",
              "      <th>movie_id</th>\n",
              "      <th>char_name</th>\n",
              "      <th>dialog</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>L1045</td>\n",
              "      <td>u0</td>\n",
              "      <td>m0</td>\n",
              "      <td>BIANCA</td>\n",
              "      <td>They do not!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>L1044</td>\n",
              "      <td>u2</td>\n",
              "      <td>m0</td>\n",
              "      <td>CAMERON</td>\n",
              "      <td>They do to!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>L985</td>\n",
              "      <td>u0</td>\n",
              "      <td>m0</td>\n",
              "      <td>BIANCA</td>\n",
              "      <td>I hope so.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>L984</td>\n",
              "      <td>u2</td>\n",
              "      <td>m0</td>\n",
              "      <td>CAMERON</td>\n",
              "      <td>She okay?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>L925</td>\n",
              "      <td>u0</td>\n",
              "      <td>m0</td>\n",
              "      <td>BIANCA</td>\n",
              "      <td>Let's go.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  line_id char_id movie_id  char_name         dialog\n",
              "0  L1045      u0       m0     BIANCA    They do not!\n",
              "1  L1044      u2       m0    CAMERON     They do to!\n",
              "2   L985      u0       m0     BIANCA      I hope so.\n",
              "3   L984      u2       m0    CAMERON       She okay?\n",
              "4   L925      u0       m0     BIANCA       Let's go."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 640
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eox4kUE0nBEO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "803f35b3-556f-4c89-ccb6-a9211e5edc02"
      },
      "source": [
        "df['line_id'] = df.line_id.apply(lambda x: x[1:]).astype(int)\r\n",
        "df = df.sort_values('line_id')\r\n",
        "df.head()"
      ],
      "execution_count": 641,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>line_id</th>\n",
              "      <th>char_id</th>\n",
              "      <th>movie_id</th>\n",
              "      <th>char_name</th>\n",
              "      <th>dialog</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>49</td>\n",
              "      <td>u0</td>\n",
              "      <td>m0</td>\n",
              "      <td>BIANCA</td>\n",
              "      <td>Did you change your hair?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>50</td>\n",
              "      <td>u3</td>\n",
              "      <td>m0</td>\n",
              "      <td>CHASTITY</td>\n",
              "      <td>No.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>51</td>\n",
              "      <td>u0</td>\n",
              "      <td>m0</td>\n",
              "      <td>BIANCA</td>\n",
              "      <td>You might wanna think about it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>648</th>\n",
              "      <td>59</td>\n",
              "      <td>u9</td>\n",
              "      <td>m0</td>\n",
              "      <td>PATRICK</td>\n",
              "      <td>I missed you.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>647</th>\n",
              "      <td>60</td>\n",
              "      <td>u8</td>\n",
              "      <td>m0</td>\n",
              "      <td>MISS PERKY</td>\n",
              "      <td>It says here you exposed yourself to a group ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     line_id  ...                                             dialog\n",
              "86        49  ...                          Did you change your hair?\n",
              "85        50  ...                                                No.\n",
              "84        51  ...                     You might wanna think about it\n",
              "648       59  ...                                      I missed you.\n",
              "647       60  ...   It says here you exposed yourself to a group ...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 641
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ido0H6RVnA_y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "147e7a9d-8bc6-48b1-f31e-9ee66a6557ab"
      },
      "source": [
        "df['reply'] = df['dialog'].shift(-1)\r\n",
        "df.head()"
      ],
      "execution_count": 642,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>line_id</th>\n",
              "      <th>char_id</th>\n",
              "      <th>movie_id</th>\n",
              "      <th>char_name</th>\n",
              "      <th>dialog</th>\n",
              "      <th>reply</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>49</td>\n",
              "      <td>u0</td>\n",
              "      <td>m0</td>\n",
              "      <td>BIANCA</td>\n",
              "      <td>Did you change your hair?</td>\n",
              "      <td>No.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>50</td>\n",
              "      <td>u3</td>\n",
              "      <td>m0</td>\n",
              "      <td>CHASTITY</td>\n",
              "      <td>No.</td>\n",
              "      <td>You might wanna think about it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>51</td>\n",
              "      <td>u0</td>\n",
              "      <td>m0</td>\n",
              "      <td>BIANCA</td>\n",
              "      <td>You might wanna think about it</td>\n",
              "      <td>I missed you.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>648</th>\n",
              "      <td>59</td>\n",
              "      <td>u9</td>\n",
              "      <td>m0</td>\n",
              "      <td>PATRICK</td>\n",
              "      <td>I missed you.</td>\n",
              "      <td>It says here you exposed yourself to a group ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>647</th>\n",
              "      <td>60</td>\n",
              "      <td>u8</td>\n",
              "      <td>m0</td>\n",
              "      <td>MISS PERKY</td>\n",
              "      <td>It says here you exposed yourself to a group ...</td>\n",
              "      <td>It was a bratwurst.  I was eating lunch.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     line_id  ...                                              reply\n",
              "86        49  ...                                                No.\n",
              "85        50  ...                     You might wanna think about it\n",
              "84        51  ...                                      I missed you.\n",
              "648       59  ...   It says here you exposed yourself to a group ...\n",
              "647       60  ...           It was a bratwurst.  I was eating lunch.\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 642
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCTLImITnA7W"
      },
      "source": [
        "max_length = 22\r\n",
        "df = df[(df['dialog'].str.len() <= max_length) & (df['reply'].str.len() <= max_length)].drop(columns=['char_id', 'movie_id', 'char_name'])\r\n"
      ],
      "execution_count": 646,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FY64Rk_nA46"
      },
      "source": [
        "df['dialog'] = df['dialog'].astype(str)\r\n",
        "df['reply'] = df['reply'].astype(str)\r\n",
        "df['line_id'] = df['line_id'].astype(int)"
      ],
      "execution_count": 647,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVc_OThuwA6h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "8f5d017e-b6ae-4abb-d11c-f090da8bc84e"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 648,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>line_id</th>\n",
              "      <th>dialog</th>\n",
              "      <th>reply</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>266</th>\n",
              "      <td>63</td>\n",
              "      <td>You the new guy?</td>\n",
              "      <td>So they tell me...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>258</th>\n",
              "      <td>71</td>\n",
              "      <td>Thirty-two.</td>\n",
              "      <td>Get out!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>385</th>\n",
              "      <td>127</td>\n",
              "      <td>He always look so</td>\n",
              "      <td>Block E?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>382</th>\n",
              "      <td>130</td>\n",
              "      <td>Just a little.</td>\n",
              "      <td>What's this?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>381</th>\n",
              "      <td>131</td>\n",
              "      <td>What's this?</td>\n",
              "      <td>An attempted slit.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     line_id              dialog                reply\n",
              "266       63    You the new guy?   So they tell me...\n",
              "258       71         Thirty-two.             Get out!\n",
              "385      127   He always look so             Block E?\n",
              "382      130      Just a little.         What's this?\n",
              "381      131        What's this?   An attempted slit."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 648
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_C1EGDxrWHb"
      },
      "source": [
        "dialog = data.Field(tokenize='spacy', init_token='<sos>', eos_token='<eos>', lower=True)\r\n",
        "reply = data.Field(tokenize='spacy', init_token='<sos>', eos_token='<eos>', lower=True)\r\n",
        "line_id = data.Field(sequential=False, use_vocab=False)\r\n",
        "fields = [('dialog', dialog), ('reply', reply), ('line_id', line_id)]\r\n",
        "example = [data.Example.fromlist([df.dialog.iloc[i], df.reply.iloc[i], df.line_id.iloc[i]], fields) for i in range(df.shape[0])]"
      ],
      "execution_count": 649,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKFds9jKuhhw"
      },
      "source": [
        "train_dset = data.Dataset(example[:int(len(df) * 0.6)], fields)\r\n",
        "test_dset = data.Dataset(example[int(len(df) * 0.6): ], fields)"
      ],
      "execution_count": 650,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsa-HKgQuhfL"
      },
      "source": [
        "dialog.build_vocab(train_dset, min_freq = 3)\r\n",
        "reply.build_vocab(test_dset, min_freq = 3)"
      ],
      "execution_count": 651,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivF8HyQyuha4"
      },
      "source": [
        "BATCH_SIZE = 256\r\n",
        "\r\n",
        "train_iterator = BucketIterator(train_dset, batch_size=BATCH_SIZE, sort=True,\r\n",
        "                           sort_key = lambda x: -x.line_id,\r\n",
        "                           sort_within_batch=True, device = device)\r\n",
        "test_iterator = BucketIterator(test_dset, batch_size=BATCH_SIZE, sort=True,\r\n",
        "                           sort_key = lambda x: -x.line_id,\r\n",
        "                           sort_within_batch=True, device = device)"
      ],
      "execution_count": 652,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfw2LpIYuhYz"
      },
      "source": [
        "class Encoder(nn.Module):\r\n",
        "    def __init__(self, \r\n",
        "                 input_dim, \r\n",
        "                 hid_dim, \r\n",
        "                 n_layers, \r\n",
        "                 n_heads, \r\n",
        "                 pf_dim,\r\n",
        "                 dropout, \r\n",
        "                 device,\r\n",
        "                 max_length = 120):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        self.device = device\r\n",
        "        \r\n",
        "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\r\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\r\n",
        "        \r\n",
        "        self.layers = nn.ModuleList([EncoderLayer(hid_dim, \r\n",
        "                                                  n_heads, \r\n",
        "                                                  pf_dim,\r\n",
        "                                                  dropout, \r\n",
        "                                                  device) \r\n",
        "                                     for _ in range(n_layers)])\r\n",
        "        \r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\r\n",
        "        \r\n",
        "    def forward(self, src, src_mask):\r\n",
        "        \r\n",
        "        #src = [batch size, src len]\r\n",
        "        #src_mask = [batch size, 1, 1, src len]\r\n",
        "        \r\n",
        "        batch_size = src.shape[0]\r\n",
        "        src_len = src.shape[1]\r\n",
        "        \r\n",
        "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\r\n",
        "        \r\n",
        "        #pos = [batch size, src len]\r\n",
        "\r\n",
        "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\r\n",
        "        \r\n",
        "        #src = [batch size, src len, hid dim]\r\n",
        "        \r\n",
        "        for layer in self.layers:\r\n",
        "            src = layer(src, src_mask)\r\n",
        "            \r\n",
        "        #src = [batch size, src len, hid dim]\r\n",
        "            \r\n",
        "        return src\r\n",
        "\r\n",
        "\r\n",
        "class EncoderLayer(nn.Module):\r\n",
        "    def __init__(self, \r\n",
        "                 hid_dim, \r\n",
        "                 n_heads, \r\n",
        "                 pf_dim,  \r\n",
        "                 dropout, \r\n",
        "                 device):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\r\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\r\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\r\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \r\n",
        "                                                                     pf_dim, \r\n",
        "                                                                     dropout)\r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "    def forward(self, src, src_mask):\r\n",
        "        \r\n",
        "        #src = [batch size, src len, hid dim]\r\n",
        "        #src_mask = [batch size, 1, 1, src len] \r\n",
        "                \r\n",
        "        #self attention\r\n",
        "        _src, _ = self.self_attention(src, src, src, src_mask)\r\n",
        "        \r\n",
        "        #dropout, residual connection and layer norm\r\n",
        "        src = self.self_attn_layer_norm(src + self.dropout(_src))\r\n",
        "        \r\n",
        "        #src = [batch size, src len, hid dim]\r\n",
        "        \r\n",
        "        #positionwise feedforward\r\n",
        "        _src = self.positionwise_feedforward(src)\r\n",
        "        \r\n",
        "        #dropout, residual and layer norm\r\n",
        "        src = self.ff_layer_norm(src + self.dropout(_src))\r\n",
        "        \r\n",
        "        #src = [batch size, src len, hid dim]\r\n",
        "        \r\n",
        "        return src"
      ],
      "execution_count": 653,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtoQ7LOZuhWa"
      },
      "source": [
        "class MultiHeadAttentionLayer(nn.Module):\r\n",
        "    def __init__(self, hid_dim, n_heads, dropout, device):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        assert hid_dim % n_heads == 0\r\n",
        "        \r\n",
        "        self.hid_dim = hid_dim\r\n",
        "        self.n_heads = n_heads\r\n",
        "        self.head_dim = hid_dim // n_heads\r\n",
        "        \r\n",
        "        self.fc_q = nn.Linear(hid_dim, hid_dim)\r\n",
        "        self.fc_k = nn.Linear(hid_dim, hid_dim)\r\n",
        "        self.fc_v = nn.Linear(hid_dim, hid_dim)\r\n",
        "        \r\n",
        "        self.fc_o = nn.Linear(hid_dim, hid_dim)\r\n",
        "        \r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\r\n",
        "        \r\n",
        "    def forward(self, query, key, value, mask = None):\r\n",
        "        \r\n",
        "        batch_size = query.shape[0]\r\n",
        "        \r\n",
        "        #query = [batch size, query len, hid dim]\r\n",
        "        #key = [batch size, key len, hid dim]\r\n",
        "        #value = [batch size, value len, hid dim]\r\n",
        "                \r\n",
        "        Q = self.fc_q(query)\r\n",
        "        K = self.fc_k(key)\r\n",
        "        V = self.fc_v(value)\r\n",
        "        \r\n",
        "        #Q = [batch size, query len, hid dim]\r\n",
        "        #K = [batch size, key len, hid dim]\r\n",
        "        #V = [batch size, value len, hid dim]\r\n",
        "                \r\n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\r\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\r\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\r\n",
        "        \r\n",
        "        #Q = [batch size, n heads, query len, head dim]\r\n",
        "        #K = [batch size, n heads, key len, head dim]\r\n",
        "        #V = [batch size, n heads, value len, head dim]\r\n",
        "                \r\n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\r\n",
        "        \r\n",
        "        #energy = [batch size, n heads, query len, key len]\r\n",
        "        \r\n",
        "        if mask is not None:\r\n",
        "            energy = energy.masked_fill(mask == 0, -1e10)\r\n",
        "        \r\n",
        "        attention = torch.softmax(energy, dim = -1)\r\n",
        "                \r\n",
        "        #attention = [batch size, n heads, query len, key len]\r\n",
        "                \r\n",
        "        x = torch.matmul(self.dropout(attention), V)\r\n",
        "        \r\n",
        "        #x = [batch size, n heads, query len, head dim]\r\n",
        "        \r\n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\r\n",
        "        \r\n",
        "        #x = [batch size, query len, n heads, head dim]\r\n",
        "        \r\n",
        "        x = x.view(batch_size, -1, self.hid_dim)\r\n",
        "        \r\n",
        "        #x = [batch size, query len, hid dim]\r\n",
        "        \r\n",
        "        x = self.fc_o(x)\r\n",
        "        \r\n",
        "        #x = [batch size, query len, hid dim]\r\n",
        "        \r\n",
        "        return x, attention\r\n",
        "\r\n",
        "class PositionwiseFeedforwardLayer(nn.Module):\r\n",
        "    def __init__(self, hid_dim, pf_dim, dropout):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\r\n",
        "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\r\n",
        "        \r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "    def forward(self, x):\r\n",
        "        \r\n",
        "        #x = [batch size, seq len, hid dim]\r\n",
        "        \r\n",
        "        x = self.dropout(torch.relu(self.fc_1(x)))\r\n",
        "        \r\n",
        "        #x = [batch size, seq len, pf dim]\r\n",
        "        \r\n",
        "        x = self.fc_2(x)\r\n",
        "        \r\n",
        "        #x = [batch size, seq len, hid dim]\r\n",
        "        \r\n",
        "        return x"
      ],
      "execution_count": 654,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axw400MqsEvy"
      },
      "source": [
        "class Decoder(nn.Module):\r\n",
        "    def __init__(self, \r\n",
        "                 output_dim, \r\n",
        "                 hid_dim, \r\n",
        "                 n_layers, \r\n",
        "                 n_heads, \r\n",
        "                 pf_dim, \r\n",
        "                 dropout, \r\n",
        "                 device,\r\n",
        "                 max_length = 120):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.device = device\r\n",
        "        \r\n",
        "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\r\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\r\n",
        "        \r\n",
        "        self.layers = nn.ModuleList([DecoderLayer(hid_dim, \r\n",
        "                                                  n_heads, \r\n",
        "                                                  pf_dim, \r\n",
        "                                                  dropout, \r\n",
        "                                                  device)\r\n",
        "                                     for _ in range(n_layers)])\r\n",
        "        \r\n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\r\n",
        "        \r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\r\n",
        "        \r\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\r\n",
        "        \r\n",
        "        #trg = [batch size, trg len]\r\n",
        "        #enc_src = [batch size, src len, hid dim]\r\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\r\n",
        "        #src_mask = [batch size, 1, 1, src len]\r\n",
        "                \r\n",
        "        batch_size = trg.shape[0]\r\n",
        "        trg_len = trg.shape[1]\r\n",
        "        \r\n",
        "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\r\n",
        "                            \r\n",
        "        #pos = [batch size, trg len]\r\n",
        "\r\n",
        "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\r\n",
        "                \r\n",
        "        #trg = [batch size, trg len, hid dim]\r\n",
        "        \r\n",
        "        for layer in self.layers:\r\n",
        "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\r\n",
        "        \r\n",
        "        #trg = [batch size, trg len, hid dim]\r\n",
        "        #attention = [batch size, n heads, trg len, src len]\r\n",
        "        \r\n",
        "        output = self.fc_out(trg)\r\n",
        "        \r\n",
        "        #output = [batch size, trg len, output dim]\r\n",
        "            \r\n",
        "        return output, attention\r\n",
        "\r\n",
        "class DecoderLayer(nn.Module):\r\n",
        "    def __init__(self, \r\n",
        "                 hid_dim, \r\n",
        "                 n_heads, \r\n",
        "                 pf_dim, \r\n",
        "                 dropout, \r\n",
        "                 device):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\r\n",
        "        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\r\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\r\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\r\n",
        "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\r\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \r\n",
        "                                                                     pf_dim, \r\n",
        "                                                                     dropout)\r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\r\n",
        "        \r\n",
        "        #trg = [batch size, trg len, hid dim]\r\n",
        "        #enc_src = [batch size, src len, hid dim]\r\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\r\n",
        "        #src_mask = [batch size, 1, 1, src len]\r\n",
        "        \r\n",
        "        #self attention\r\n",
        "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\r\n",
        "        \r\n",
        "        #dropout, residual connection and layer norm\r\n",
        "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\r\n",
        "            \r\n",
        "        #trg = [batch size, trg len, hid dim]\r\n",
        "            \r\n",
        "        #encoder attention\r\n",
        "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\r\n",
        "        # query, key, value\r\n",
        "        \r\n",
        "        #dropout, residual connection and layer norm\r\n",
        "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\r\n",
        "                    \r\n",
        "        #trg = [batch size, trg len, hid dim]\r\n",
        "        \r\n",
        "        #positionwise feedforward\r\n",
        "        _trg = self.positionwise_feedforward(trg)\r\n",
        "        \r\n",
        "        #dropout, residual and layer norm\r\n",
        "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\r\n",
        "        \r\n",
        "        #trg = [batch size, trg len, hid dim]\r\n",
        "        #attention = [batch size, n heads, trg len, src len]\r\n",
        "        \r\n",
        "        return trg, attention"
      ],
      "execution_count": 655,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwZ-GH1KwwjB"
      },
      "source": [
        "class Seq2Seq(nn.Module):\r\n",
        "    def __init__(self, \r\n",
        "                 encoder, \r\n",
        "                 decoder, \r\n",
        "                 src_pad_idx, \r\n",
        "                 trg_pad_idx, \r\n",
        "                 device):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.encoder = encoder\r\n",
        "        self.decoder = decoder\r\n",
        "        self.src_pad_idx = src_pad_idx\r\n",
        "        self.trg_pad_idx = trg_pad_idx\r\n",
        "        self.device = device\r\n",
        "        \r\n",
        "    def make_src_mask(self, src):\r\n",
        "        \r\n",
        "        #src = [batch size, src len]\r\n",
        "        \r\n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\r\n",
        "\r\n",
        "        #src_mask = [batch size, 1, 1, src len]\r\n",
        "\r\n",
        "        return src_mask\r\n",
        "    \r\n",
        "    def make_trg_mask(self, trg):\r\n",
        "        \r\n",
        "        #trg = [batch size, trg len]\r\n",
        "        \r\n",
        "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\r\n",
        "        \r\n",
        "        #trg_pad_mask = [batch size, 1, 1, trg len]\r\n",
        "        \r\n",
        "        trg_len = trg.shape[1]\r\n",
        "        \r\n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\r\n",
        "        \r\n",
        "        #trg_sub_mask = [trg len, trg len]\r\n",
        "            \r\n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\r\n",
        "        \r\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\r\n",
        "        \r\n",
        "        return trg_mask\r\n",
        "\r\n",
        "    def forward(self, src, trg):\r\n",
        "        \r\n",
        "        #src = [batch size, src len]\r\n",
        "        #trg = [batch size, trg len]\r\n",
        "                \r\n",
        "        src_mask = self.make_src_mask(src)\r\n",
        "        trg_mask = self.make_trg_mask(trg)\r\n",
        "        \r\n",
        "        #src_mask = [batch size, 1, 1, src len]\r\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\r\n",
        "        \r\n",
        "        enc_src = self.encoder(src, src_mask)\r\n",
        "        \r\n",
        "        #enc_src = [batch size, src len, hid dim]\r\n",
        "                \r\n",
        "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\r\n",
        "        \r\n",
        "        #output = [batch size, trg len, output dim]\r\n",
        "        #attention = [batch size, n heads, trg len, src len]\r\n",
        "        #return output, attention, mask_\r\n",
        "        return output, attention\r\n"
      ],
      "execution_count": 656,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJLrAGitw0zR"
      },
      "source": [
        "INPUT_DIM = len(dialog.vocab)\r\n",
        "OUTPUT_DIM = len(reply.vocab)\r\n",
        "HID_DIM = 256\r\n",
        "ENC_LAYERS = 3\r\n",
        "DEC_LAYERS = 3\r\n",
        "ENC_HEADS = 8\r\n",
        "DEC_HEADS = 8\r\n",
        "ENC_PF_DIM = 512\r\n",
        "DEC_PF_DIM = 512\r\n",
        "ENC_DROPOUT = 0.10\r\n",
        "DEC_DROPOUT = 0.10\r\n",
        "\r\n",
        "enc = Encoder(INPUT_DIM, \r\n",
        "              HID_DIM, \r\n",
        "              ENC_LAYERS, \r\n",
        "              ENC_HEADS, \r\n",
        "              ENC_PF_DIM, \r\n",
        "              ENC_DROPOUT, \r\n",
        "              device)\r\n",
        "\r\n",
        "dec = Decoder(OUTPUT_DIM, \r\n",
        "              HID_DIM, \r\n",
        "              DEC_LAYERS, \r\n",
        "              DEC_HEADS, \r\n",
        "              DEC_PF_DIM, \r\n",
        "              DEC_DROPOUT,\r\n",
        "              device)"
      ],
      "execution_count": 657,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjT7DDL6w1f6"
      },
      "source": [
        "SRC_PAD_IDX = dialog.vocab.stoi[dialog.pad_token]\r\n",
        "TRG_PAD_IDX = reply.vocab.stoi[reply.pad_token]\r\n",
        "\r\n",
        "model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
      ],
      "execution_count": 658,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiapfU-Nw1d_"
      },
      "source": [
        "def initialize_weights(m):\r\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\r\n",
        "        nn.init.xavier_uniform_(m.weight.data)\r\n",
        "model.apply(initialize_weights);"
      ],
      "execution_count": 659,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSVpY0Zdw1bu"
      },
      "source": [
        "LEARNING_RATE = 0.0001\r\n",
        "\r\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE, weight_decay=1e-6)\r\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor=0.5, patience=5)\r\n"
      ],
      "execution_count": 660,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnCMnUz0w1Zm"
      },
      "source": [
        "# def maskNLLLoss(inp, target, mask):\r\n",
        "#     nTotal = mask.sum()\r\n",
        "#     #print(f'target shape : {target.shape}, mask shape : {mask.shape}')\r\n",
        "#     crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\r\n",
        "#     loss = crossEntropy.masked_select(mask.reshape(-1)).mean()\r\n",
        "#     #loss = loss_[~torch.isnan(loss_)].mean()\r\n",
        "#     loss = loss.to(device)\r\n",
        "#     return loss, nTotal.item()\r\n",
        "\r\n",
        "def maskNLLLoss(inp, target, mask):\r\n",
        "    # print(inp.shape, target.shape, mask.sum())\r\n",
        "    nTotal = mask.sum()\r\n",
        "    crossEntropy = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\r\n",
        "    loss = crossEntropy(inp, target)\r\n",
        "    loss = loss.to(device)\r\n",
        "    return loss, nTotal.item()"
      ],
      "execution_count": 661,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJoVvgX3w1Xe"
      },
      "source": [
        "criterion = maskNLLLoss\r\n"
      ],
      "execution_count": 662,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfjoKPYjw1VA"
      },
      "source": [
        "from tqdm import tqdm\r\n",
        "from functools import partial \r\n",
        "\r\n",
        "tqdm = partial(tqdm, position=0, leave=True)\r\n",
        "def make_trg_mask(trg):\r\n",
        "        \r\n",
        "        #trg = [batch size, trg len]\r\n",
        "        \r\n",
        "        trg_pad_mask = (trg != TRG_PAD_IDX).unsqueeze(1).unsqueeze(2)\r\n",
        "        \r\n",
        "        #trg_pad_mask = [batch size, 1, 1, trg len]\r\n",
        "        \r\n",
        "        trg_len = trg.shape[1]\r\n",
        "        \r\n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = device)).bool()\r\n",
        "        \r\n",
        "        #trg_sub_mask = [trg len, trg len]\r\n",
        "            \r\n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\r\n",
        "        \r\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\r\n",
        "        \r\n",
        "        return trg_mask\r\n",
        "\r\n",
        "def train(model, iterator, optimizer, criterion, clip):\r\n",
        "    \r\n",
        "    model.train()\r\n",
        "    \r\n",
        "    n_totals = 0\r\n",
        "    print_losses = []\r\n",
        "    for i, batch in tqdm(enumerate(iterator), total=len(iterator)):\r\n",
        "        # print(batch)\r\n",
        "        \r\n",
        "        loss = 0\r\n",
        "        src = batch.dialog.permute(1, 0)\r\n",
        "        trg = batch.reply.permute(1, 0)\r\n",
        "        trg_mask = make_trg_mask(trg)\r\n",
        "        optimizer.zero_grad()\r\n",
        "        \r\n",
        "        output, _ = model(src, trg[:,:-1])\r\n",
        "                \r\n",
        "        #output = [batch size, trg len - 1, output dim]\r\n",
        "        #trg = [batch size, trg len]\r\n",
        "            \r\n",
        "        output_dim = output.shape[-1]\r\n",
        "            \r\n",
        "        output = output.contiguous().view(-1, output_dim)\r\n",
        "        trg = trg[:,1:].contiguous().view(-1)\r\n",
        "                \r\n",
        "        #output = [batch size * trg len - 1, output dim]\r\n",
        "        #trg = [batch size * trg len - 1]\r\n",
        "        mask_loss, nTotal = criterion(output, trg, trg_mask)\r\n",
        "        \r\n",
        "        mask_loss.backward()\r\n",
        "        \r\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\r\n",
        "        \r\n",
        "        optimizer.step()\r\n",
        "       \r\n",
        "          \r\n",
        "        print_losses.append(mask_loss.item() * nTotal)\r\n",
        "        n_totals += nTotal\r\n",
        "\r\n",
        "\r\n",
        "        \r\n",
        "    return sum(print_losses) / n_totals\r\n",
        "\r\n",
        "def evaluate(model, iterator, criterion):\r\n",
        "    \r\n",
        "    model.eval()\r\n",
        "    \r\n",
        "    n_totals = 0\r\n",
        "    print_losses = []\r\n",
        "    \r\n",
        "    with torch.no_grad():\r\n",
        "    \r\n",
        "        for i, batch in tqdm(enumerate(iterator), total=len(iterator)):\r\n",
        "\r\n",
        "            src = batch.dialog.permute(1, 0)\r\n",
        "            trg = batch.reply.permute(1, 0)\r\n",
        "            trg_mask = make_trg_mask(trg)\r\n",
        "\r\n",
        "            output, _ = model(src, trg[:,:-1])\r\n",
        "            \r\n",
        "            #output = [batch size, trg len - 1, output dim]\r\n",
        "            #trg = [batch size, trg len]\r\n",
        "            \r\n",
        "            output_dim = output.shape[-1]\r\n",
        "            \r\n",
        "            output = output.contiguous().view(-1, output_dim)\r\n",
        "            trg = trg[:,1:].contiguous().view(-1)\r\n",
        "            \r\n",
        "            #output = [batch size * trg len - 1, output dim]\r\n",
        "            #trg = [batch size * trg len - 1]\r\n",
        "            \r\n",
        "            mask_loss, nTotal = criterion(output, trg, trg_mask)\r\n",
        "\r\n",
        "            print_losses.append(mask_loss.item() * nTotal)\r\n",
        "            n_totals += nTotal\r\n",
        "\r\n",
        "\r\n",
        "        \r\n",
        "    return sum(print_losses) / n_totals"
      ],
      "execution_count": 663,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-N9r8h5xnri"
      },
      "source": [
        "def epoch_time(start_time, end_time):\r\n",
        "    elapsed_time = end_time - start_time\r\n",
        "    elapsed_mins = int(elapsed_time / 60)\r\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\r\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 664,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XN1_QfKzxpvu",
        "outputId": "7fe75f68-87ba-4bc6-fa3d-20b9a4ed8e83"
      },
      "source": [
        "N_EPOCHS = 25\r\n",
        "CLIP = 1\r\n",
        "best_valid_loss = float('inf')\r\n",
        "\r\n",
        "for epoch in range(N_EPOCHS):\r\n",
        "    \r\n",
        "    start_time = time.time()\r\n",
        "    print(f'LR before train() :- {optimizer.param_groups[0][\"lr\"]}\\n')\r\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\r\n",
        "    print(f'LR after train():- {optimizer.param_groups[0][\"lr\"]}\\n')\r\n",
        "    \r\n",
        "    valid_loss = evaluate(model, test_iterator, criterion)\r\n",
        "    if scheduler:\r\n",
        "      scheduler.step(valid_loss)\r\n",
        "    end_time = time.time()\r\n",
        "    \r\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n",
        "    \r\n",
        "    # if valid_loss < best_valid_loss:\r\n",
        "    #     best_valid_loss = valid_loss\r\n",
        "    #     torch.save(model.state_dict(), 'tut6-model.pt')\r\n",
        "    \r\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\r\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\r\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": 665,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  1%|▏         | 1/68 [00:00<00:08,  8.35it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LR before train() :- 0.0001\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:04<00:00, 14.96it/s]\n",
            " 15%|█▌        | 7/46 [00:00<00:00, 61.60it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LR after train():- 0.0001\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 46/46 [00:00<00:00, 63.34it/s]\n",
            "  3%|▎         | 2/68 [00:00<00:04, 14.14it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 5s\n",
            "\tTrain Loss: 4.044 | Train PPL:  57.033\n",
            "\t Val. Loss: 3.109 |  Val. PPL:  22.389\n",
            "LR before train() :- 0.0001\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:04<00:00, 15.53it/s]\n",
            " 15%|█▌        | 7/46 [00:00<00:00, 62.79it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LR after train():- 0.0001\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 46/46 [00:00<00:00, 63.58it/s]\n",
            "  3%|▎         | 2/68 [00:00<00:04, 14.11it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 02 | Time: 0m 5s\n",
            "\tTrain Loss: 2.857 | Train PPL:  17.408\n",
            "\t Val. Loss: 2.783 |  Val. PPL:  16.175\n",
            "LR before train() :- 0.0001\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:04<00:00, 15.56it/s]\n",
            " 15%|█▌        | 7/46 [00:00<00:00, 62.87it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LR after train():- 0.0001\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 46/46 [00:00<00:00, 63.77it/s]\n",
            "  3%|▎         | 2/68 [00:00<00:04, 14.07it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 03 | Time: 0m 5s\n",
            "\tTrain Loss: 2.633 | Train PPL:  13.909\n",
            "\t Val. Loss: 2.666 |  Val. PPL:  14.381\n",
            "LR before train() :- 0.0001\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:04<00:00, 15.58it/s]\n",
            " 15%|█▌        | 7/46 [00:00<00:00, 62.22it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LR after train():- 0.0001\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 46/46 [00:00<00:00, 62.71it/s]\n",
            "  3%|▎         | 2/68 [00:00<00:04, 13.57it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 04 | Time: 0m 5s\n",
            "\tTrain Loss: 2.525 | Train PPL:  12.494\n",
            "\t Val. Loss: 2.603 |  Val. PPL:  13.501\n",
            "LR before train() :- 0.0001\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:04<00:00, 15.50it/s]\n",
            " 15%|█▌        | 7/46 [00:00<00:00, 62.57it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LR after train():- 0.0001\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 46/46 [00:00<00:00, 62.91it/s]\n",
            "  3%|▎         | 2/68 [00:00<00:04, 14.34it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 05 | Time: 0m 5s\n",
            "\tTrain Loss: 2.454 | Train PPL:  11.630\n",
            "\t Val. Loss: 2.554 |  Val. PPL:  12.859\n",
            "LR before train() :- 0.0001\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:04<00:00, 15.57it/s]\n",
            " 13%|█▎        | 6/46 [00:00<00:00, 58.45it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LR after train():- 0.0001\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 46/46 [00:00<00:00, 62.14it/s]\n",
            "  3%|▎         | 2/68 [00:00<00:04, 14.15it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 06 | Time: 0m 5s\n",
            "\tTrain Loss: 2.398 | Train PPL:  10.999\n",
            "\t Val. Loss: 2.516 |  Val. PPL:  12.383\n",
            "LR before train() :- 0.0001\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:04<00:00, 15.56it/s]\n",
            " 15%|█▌        | 7/46 [00:00<00:00, 62.41it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LR after train():- 0.0001\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 46/46 [00:00<00:00, 62.29it/s]\n",
            "  3%|▎         | 2/68 [00:00<00:04, 14.17it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 07 | Time: 0m 5s\n",
            "\tTrain Loss: 2.350 | Train PPL:  10.484\n",
            "\t Val. Loss: 2.488 |  Val. PPL:  12.036\n",
            "LR before train() :- 0.0001\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:04<00:00, 15.62it/s]\n",
            " 15%|█▌        | 7/46 [00:00<00:00, 61.79it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LR after train():- 0.0001\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 46/46 [00:00<00:00, 62.20it/s]\n",
            "  3%|▎         | 2/68 [00:00<00:04, 13.48it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 08 | Time: 0m 5s\n",
            "\tTrain Loss: 2.309 | Train PPL:  10.061\n",
            "\t Val. Loss: 2.465 |  Val. PPL:  11.764\n",
            "LR before train() :- 0.0001\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:04<00:00, 15.56it/s]\n",
            " 13%|█▎        | 6/46 [00:00<00:00, 58.81it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LR after train():- 0.0001\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 46/46 [00:00<00:00, 61.48it/s]\n",
            "  3%|▎         | 2/68 [00:00<00:04, 13.81it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 09 | Time: 0m 5s\n",
            "\tTrain Loss: 2.269 | Train PPL:   9.674\n",
            "\t Val. Loss: 2.448 |  Val. PPL:  11.560\n",
            "LR before train() :- 0.0001\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:04<00:00, 15.54it/s]\n",
            " 15%|█▌        | 7/46 [00:00<00:00, 62.25it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LR after train():- 0.0001\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 46/46 [00:00<00:00, 62.01it/s]\n",
            "  3%|▎         | 2/68 [00:00<00:04, 14.17it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 10 | Time: 0m 5s\n",
            "\tTrain Loss: 2.236 | Train PPL:   9.359\n",
            "\t Val. Loss: 2.432 |  Val. PPL:  11.378\n",
            "LR before train() :- 0.0001\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:04<00:00, 15.54it/s]\n",
            " 15%|█▌        | 7/46 [00:00<00:00, 61.30it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LR after train():- 0.0001\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 46/46 [00:00<00:00, 62.08it/s]\n",
            "  3%|▎         | 2/68 [00:00<00:04, 14.36it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 11 | Time: 0m 5s\n",
            "\tTrain Loss: 2.206 | Train PPL:   9.079\n",
            "\t Val. Loss: 2.422 |  Val. PPL:  11.271\n",
            "LR before train() :- 0.0001\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:04<00:00, 15.51it/s]\n",
            " 15%|█▌        | 7/46 [00:00<00:00, 61.93it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LR after train():- 0.0001\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 46/46 [00:00<00:00, 62.67it/s]\n",
            "  3%|▎         | 2/68 [00:00<00:04, 14.01it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 12 | Time: 0m 5s\n",
            "\tTrain Loss: 2.177 | Train PPL:   8.820\n",
            "\t Val. Loss: 2.416 |  Val. PPL:  11.198\n",
            "LR before train() :- 0.0001\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:04<00:00, 15.42it/s]\n",
            " 13%|█▎        | 6/46 [00:00<00:00, 59.74it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LR after train():- 0.0001\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 46/46 [00:00<00:00, 61.73it/s]\n",
            "  3%|▎         | 2/68 [00:00<00:04, 13.64it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 13 | Time: 0m 5s\n",
            "\tTrain Loss: 2.145 | Train PPL:   8.546\n",
            "\t Val. Loss: 2.409 |  Val. PPL:  11.124\n",
            "LR before train() :- 0.0001\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:04<00:00, 15.35it/s]\n",
            " 13%|█▎        | 6/46 [00:00<00:00, 59.89it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LR after train():- 0.0001\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 46/46 [00:00<00:00, 61.64it/s]\n",
            "  3%|▎         | 2/68 [00:00<00:04, 13.87it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 14 | Time: 0m 5s\n",
            "\tTrain Loss: 2.121 | Train PPL:   8.341\n",
            "\t Val. Loss: 2.405 |  Val. PPL:  11.080\n",
            "LR before train() :- 0.0001\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:04<00:00, 15.46it/s]\n",
            " 13%|█▎        | 6/46 [00:00<00:00, 58.92it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LR after train():- 0.0001\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 46/46 [00:00<00:00, 61.80it/s]\n",
            "  3%|▎         | 2/68 [00:00<00:04, 14.05it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 15 | Time: 0m 5s\n",
            "\tTrain Loss: 2.098 | Train PPL:   8.153\n",
            "\t Val. Loss: 2.400 |  Val. PPL:  11.019\n",
            "LR before train() :- 0.0001\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:04<00:00, 15.32it/s]\n",
            " 13%|█▎        | 6/46 [00:00<00:00, 58.65it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LR after train():- 0.0001\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 46/46 [00:00<00:00, 61.80it/s]\n",
            "  3%|▎         | 2/68 [00:00<00:04, 14.07it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 16 | Time: 0m 5s\n",
            "\tTrain Loss: 2.074 | Train PPL:   7.953\n",
            "\t Val. Loss: 2.401 |  Val. PPL:  11.033\n",
            "LR before train() :- 0.0001\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:04<00:00, 15.34it/s]\n",
            " 15%|█▌        | 7/46 [00:00<00:00, 60.95it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LR after train():- 0.0001\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 46/46 [00:00<00:00, 61.69it/s]\n",
            "  3%|▎         | 2/68 [00:00<00:04, 13.55it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 17 | Time: 0m 5s\n",
            "\tTrain Loss: 2.049 | Train PPL:   7.760\n",
            "\t Val. Loss: 2.397 |  Val. PPL:  10.992\n",
            "LR before train() :- 0.0001\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:04<00:00, 15.19it/s]\n",
            " 13%|█▎        | 6/46 [00:00<00:00, 58.99it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LR after train():- 0.0001\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 46/46 [00:00<00:00, 61.40it/s]\n",
            "  3%|▎         | 2/68 [00:00<00:04, 14.21it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 18 | Time: 0m 5s\n",
            "\tTrain Loss: 2.027 | Train PPL:   7.594\n",
            "\t Val. Loss: 2.396 |  Val. PPL:  10.985\n",
            "LR before train() :- 0.0001\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:04<00:00, 15.43it/s]\n",
            " 15%|█▌        | 7/46 [00:00<00:00, 61.03it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LR after train():- 0.0001\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 46/46 [00:00<00:00, 61.52it/s]\n",
            "  3%|▎         | 2/68 [00:00<00:04, 13.74it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 19 | Time: 0m 5s\n",
            "\tTrain Loss: 2.005 | Train PPL:   7.428\n",
            "\t Val. Loss: 2.400 |  Val. PPL:  11.020\n",
            "LR before train() :- 0.0001\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:04<00:00, 15.29it/s]\n",
            " 13%|█▎        | 6/46 [00:00<00:00, 59.43it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LR after train():- 0.0001\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 46/46 [00:00<00:00, 61.68it/s]\n",
            "  3%|▎         | 2/68 [00:00<00:04, 14.14it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 20 | Time: 0m 5s\n",
            "\tTrain Loss: 1.986 | Train PPL:   7.286\n",
            "\t Val. Loss: 2.404 |  Val. PPL:  11.072\n",
            "LR before train() :- 0.0001\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:04<00:00, 15.30it/s]\n",
            " 13%|█▎        | 6/46 [00:00<00:00, 59.54it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LR after train():- 0.0001\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 46/46 [00:00<00:00, 61.22it/s]\n",
            "  3%|▎         | 2/68 [00:00<00:04, 13.79it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 21 | Time: 0m 5s\n",
            "\tTrain Loss: 1.965 | Train PPL:   7.132\n",
            "\t Val. Loss: 2.405 |  Val. PPL:  11.077\n",
            "LR before train() :- 0.0001\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:04<00:00, 15.24it/s]\n",
            " 13%|█▎        | 6/46 [00:00<00:00, 55.94it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LR after train():- 0.0001\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 46/46 [00:00<00:00, 60.43it/s]\n",
            "  3%|▎         | 2/68 [00:00<00:04, 13.79it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 22 | Time: 0m 5s\n",
            "\tTrain Loss: 1.946 | Train PPL:   6.998\n",
            "\t Val. Loss: 2.407 |  Val. PPL:  11.104\n",
            "LR before train() :- 0.0001\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:04<00:00, 15.23it/s]\n",
            " 13%|█▎        | 6/46 [00:00<00:00, 59.19it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LR after train():- 0.0001\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 46/46 [00:00<00:00, 61.13it/s]\n",
            "  3%|▎         | 2/68 [00:00<00:04, 13.99it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 23 | Time: 0m 5s\n",
            "\tTrain Loss: 1.926 | Train PPL:   6.862\n",
            "\t Val. Loss: 2.414 |  Val. PPL:  11.176\n",
            "LR before train() :- 0.0001\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:04<00:00, 15.14it/s]\n",
            " 13%|█▎        | 6/46 [00:00<00:00, 59.62it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LR after train():- 0.0001\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 46/46 [00:00<00:00, 60.45it/s]\n",
            "  3%|▎         | 2/68 [00:00<00:04, 13.86it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 24 | Time: 0m 5s\n",
            "\tTrain Loss: 1.907 | Train PPL:   6.731\n",
            "\t Val. Loss: 2.420 |  Val. PPL:  11.250\n",
            "LR before train() :- 5e-05\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:04<00:00, 15.14it/s]\n",
            " 15%|█▌        | 7/46 [00:00<00:00, 61.05it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LR after train():- 5e-05\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 46/46 [00:00<00:00, 61.51it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 25 | Time: 0m 5s\n",
            "\tTrain Loss: 1.884 | Train PPL:   6.577\n",
            "\t Val. Loss: 2.422 |  Val. PPL:  11.263\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zUIFm3OBiN_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}